server:
  port: 8080

spring:
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.jdbc.Driver
    url: jdbc:mysql://localhost:3306/demo?serverTimezone=UTC&characterEncoding=utf8&useUnicode=true&useSSL=false&allowPublicKeyRetrieval=true
    username: root
    password: 123456

  redis:
    database: 0
    host: 127.0.0.1
    port: 6379
    password: 123456
    timeout: 1000
    max-active: 8
    max-wait: -1
    max-idle: 8
    min-idle: 0

  kafka:
    # kafka集群数量
    # 用于配置 replicationFactor : 显示创建 topic 时,设置的 partition 副本基数
    broker-count: 1
    # Kafka 集群信息
    bootstrap-servers:
      - 127.0.0.1:9092
#      - 192.168.128.3:9093
#      - 192.168.128.3:9094
    listener:
      # 消费者确认模式: 手动
      ack-mode: manual_immediate
      # 开启多少个消费者线程
      concurrency: 3
      # 一次确认消费的数量
      ack-count: 1
      type: single
      ack-time: 60
    template:
      # 默认 topic 名称
      default-topic: dankal-test-default-topic
    consumer:
      # 采用手动提交
      enable-auto-commit: false
      # 消息使用 Spring 提供的 JSON 发序列化器
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      # 设置为最初的消费进度
      # 其取值具体区别可以参阅
      # https://blog.csdn.net/lishuangzhe7047/article/details/74530417
      auto-offset-reset: earliest
      properties:
        # 默认信任 java.util,java.lang 包下面的类
        # 因此我们需要添加 spring json trusted 配置
        spring:
          json:
            trusted:
              packages: com.TandK.model.bo
        session:
          timeout:
            ms: 10000
        max:
          poll:
            interval:
              ms: 300000
    producer:
      # 发送失败,重试次数
      retries: 3
      # 只需要 leader 应答即可
      acks: 1
      # 消息内容使用 Spring 提供的 JSON 序列化器
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

#  kafka:
#    bootstrap-servers: 127.0.0.1:9092
#    producer:
#      key-serializer: org.springframework.kafka.support.serializer.JsonDeserializer
#      value-serializer: org.springframework.kafka.support.serializer.JsonDeserializer
#    consumer:
#      group-id: group_id
#      auto-offset-reset: earliest
#      key-serializer: org.apache.kafka.common.serialization.StringSerializer
#      value-serializer: org.apache.kafka.common.serialization.StringSerializer
#  template:
#    default-topic: kafka-test  #默认topic名称可以与logstash的input的名称一致


dankal:
  distribute:
    cache:
      spin-sleep-time-in-second: 300
      avalanche-random-cardinal: 30

# sql打印
logging:
  level:
    com:
      TandK:
        mapper: debug
